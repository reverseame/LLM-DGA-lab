"""
Analyzer module for LLM-based Algorithmically Generated Domain (AGD) detection.

This module provides comprehensive analysis capabilities for evaluating the performance
of Large Language Models in detecting and classifying malicious domains generated by
Domain Generation Algorithms (DGAs).

Features:
- Binary classification analysis (malicious vs benign)
- Multiclass family classification analysis  
- Real-world domain analysis
- Confidence interval calculations
- ROC analysis support
- Visualization generation
"""

import os
import math
import matplotlib.pyplot as plt
import numpy as np

from collections import defaultdict
from typing import Dict, List, Tuple, Optional

from utils.metrics import Metrics
from utils.file_utils import save_to_text_file


class Analyzer:
    """
    Main analyzer class for evaluating LLM performance on AGD detection tasks.
    
    This class handles different types of analyses:
    1. Binary classification (Experiments 1-2): Malicious vs Benign domains
    2. Multiclass classification (Experiment 3): Classification by malware family
    3. Real-world analysis (Experiment 4): Performance on real DNS traffic
    """

    def __init__(self, malicious_dir: str, benign_file: str):
        """
        Initializes the domain classifier analyzer.

        Args:
            malicious_dir (str): Path to directory containing malicious domain files (one per family)
            benign_file (str): Path to file containing benign/legitimate domains
        """
        self.malicious_dir = malicious_dir
        self.benign_file = benign_file
        self.format_error_file = "format_error.txt"
        self.format_error_multiclass_file = "format_error_multiclass.txt"

    # ========================================================================
    # CORE UTILITY METHODS
    # ========================================================================

    def validate_domains(self, domains: List[str], classifications: List[str], 
                        output_file: str) -> bool:
        """
        Validates that all domains have corresponding classifications.
        
        This method checks if every domain in the input list has a corresponding
        classification result, which is crucial for ensuring complete evaluation.

        Args:
            domains (List[str]): List of domains to validate
            classifications (List[str]): List of classification results in format "[DOMAIN]|[RESULT]|..."
            output_file (str): Path to save any missing domains

        Returns:
            bool: True if all domains are classified, False if some are missing
        """
        # Extract domain names from classifications for fast lookup
        classification_domains = {entry.split("|")[0] for entry in classifications}

        missing_domains = []
        unique_domains = list(set(domains))  # Remove duplicates

        for domain in unique_domains:
            if domain not in classification_domains:
                missing_domains.append(domain)

        # Save missing domains if any, or remove output file if all are classified
        if missing_domains:
            save_to_text_file(filepath=output_file, data=missing_domains)
            return False
        else:
            if os.path.exists(output_file):
                os.remove(output_file)
            return True

    def read_file(self, file_path: str) -> Tuple[List[str], List[str]]:
        """
        Reads and parses LLM output files.
        
        The expected file format follows the structure:
        [domains,to,classify]
        ---------------
        [classification results]
        ***************
        [next batch...]

        Args:
            file_path (str): Path to the file containing LLM results

        Returns:
            Tuple[List[str], List[str]]: Tuple containing:
                - List of domains that were sent to the LLM
                - List of classification results from the LLM

        Raises:
            FileNotFoundError: If the specified file doesn't exist
            Exception: For other file reading errors
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()
        except FileNotFoundError:
            raise FileNotFoundError(f"The file '{file_path}' does not exist.")
        except Exception as e:
            raise Exception(f"Error reading the file '{file_path}': {e}")

        # Parse the structured output format
        blocks = content.split("***************")
        classifications = []
        domains = []

        for block in blocks:
            if "---------------" in block:
                try:
                    # Extract domains (before separator)
                    before_separator = block.split("---------------", 1)[0]
                    domain_list = [d.strip() for d in before_separator.strip().split(",")]
                    domains.extend(domain_list)
                    
                    # Extract classifications (after separator)
                    after_separator = block.split("---------------", 1)[1]
                    classification_list = [line.strip() for line in after_separator.strip().splitlines() if line.strip()]
                    classifications.extend(classification_list)
                except IndexError:
                    continue

        return domains, classifications

    def compute_metrics(self, tp: int, fp: int, fn: int, tn: int) -> Metrics:
        """
        Computes comprehensive performance metrics from confusion matrix values.
        
        This method calculates all standard classification metrics used in the paper:
        - Accuracy, Precision, Recall, F1-score
        - False Positive Rate (FPR), True Positive Rate (TPR)  
        - Matthews Correlation Coefficient (MCC)
        - Cohen's Kappa coefficient

        Args:
            tp (int): True Positives
            fp (int): False Positives  
            fn (int): False Negatives
            tn (int): True Negatives

        Returns:
            Metrics: Instance containing all calculated metrics
        """
        total = tp + fp + fn + tn
        
        # Basic metrics
        accuracy = (tp + tn) / total if total > 0 else 0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        # Rate metrics
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
        tpr = recall  # TPR is the same as recall
        
        # Advanced metrics
        # Matthews Correlation Coefficient (MCC)
        mcc_denominator = math.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))
        if mcc_denominator != 0:
            mcc = ((tp * tn) - (fp * fn)) / mcc_denominator
        else:
            mcc = 0
        
        # Cohen's Kappa
        if total != 0:
            p0 = (tn + tp) / total  # Observed agreement
            pa = ((tn + tp) / total) * ((tn + fn) / total)  # Expected agreement for negative class
            pb = ((fn + tp) / total) * ((fp + tp) / total)  # Expected agreement for positive class
            pe = pa + pb  # Total expected agreement
            kappa = (p0 - pe) / (1 - pe) if pe != 1 else 0
        else:
            kappa = 0

        return Metrics(accuracy, precision, recall, f1_score, fpr, tpr, mcc, kappa)

    # ========================================================================
    # BINARY CLASSIFICATION ANALYSIS (Experiments 1-2)
    # ========================================================================

    def analyze_classifier(self, result_list: List[str]) -> Tuple[Metrics, Metrics, Metrics]:
        """
        Analyzes binary classification results and computes metrics.
        
        This method is used for Experiments 1 and 2 where domains are classified
        as either malicious (Y) or benign (N).

        Args:
            result_list (List[str]): List of classification results in format "[DOMAIN]|[Y/N]|[CONFIDENCE]"

        Returns:
            Tuple[Metrics, Metrics, Metrics]: Metrics for (malicious_domains, benign_domains, overall)
        """
        # Load ground truth data
        malicious_domains = set()
        benign_domains = set()
        analyzed_domains = set()

        # Load malicious domains from all family files
        for filename in os.listdir(self.malicious_dir):
            file_path = os.path.join(self.malicious_dir, filename)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    domains = [line.strip() for line in f if line.strip()]
                    malicious_domains.update(domains)
            except Exception as e:
                print(f"Warning: Could not read {filename}: {e}")

        # Load benign domains
        try:
            with open(self.benign_file, 'r', encoding='utf-8') as f:
                domains = [line.strip() for line in f if line.strip()]
                benign_domains.update(domains)
        except Exception as e:
            print(f"Warning: Could not read benign file: {e}")

        # Initialize confusion matrix
        tp, fn, tn, fp = 0, 0, 0, 0

        # Process classification results
        for result in result_list:
            try:
                parts = result.split('|')
                if len(parts) < 3:
                    raise ValueError("Insufficient parts in result")
                    
                domain, classification = parts[0].strip(), parts[1].strip()
                
                # Skip duplicates
                if domain in analyzed_domains:
                    continue
                    
                # Classify based on ground truth
                if domain in malicious_domains:
                    if classification == 'Y':  # Correctly identified as malicious
                        tp += 1
                    else:  # Missed malicious domain
                        fn += 1
                    analyzed_domains.add(domain)
                elif domain in benign_domains:
                    if classification == 'N':  # Correctly identified as benign
                        tn += 1
                    else:  # False alarm
                        fp += 1
                    analyzed_domains.add(domain)
                # Ignore domains not in our datasets
                    
            except (ValueError, IndexError) as e:
                # Log formatting errors
                with open(self.format_error_file, 'a', encoding='utf-8') as error_file:
                    error_file.write(f"Format error: {result}\n")
                continue

        # Compute metrics for different domain types
        malicious_metrics = self.compute_metrics(tp, 0, fn, 0)  # Only TP and FN for malicious
        benign_metrics = self.compute_metrics(tn, 0, fp, 0)     # Only TN and FP for benign  
        overall_metrics = self.compute_metrics(tp, fp, fn, tn)   # Full confusion matrix

        return malicious_metrics, benign_metrics, overall_metrics

    # ========================================================================
    # MULTICLASS FAMILY CLASSIFICATION ANALYSIS (Experiment 3)
    # ========================================================================

    def analyze_family_metrics(self, result_list: List[str]) -> Tuple[Dict, Dict]:
        """
        Analyzes multiclass family classification results.
        
        This method is used for Experiment 3 where malicious domains are classified
        by their generating malware family.

        Args:
            result_list (List[str]): List of results in format "[DOMAIN]|[Y/N]|[FAMILY]|[CONFIDENCE]"

        Returns:
            Tuple[Dict, Dict]: (family_specific_metrics, global_metrics)
        """
        # Initialize family statistics
        family_stats = defaultdict(lambda: {
            "true_positives": 0, "false_positives": 0, 
            "false_negatives": 0, "true_negatives": 0,
            "hits": 0, "misses": 0,
            "accuracy": 0.0, "precision": 0.0, "recall": 0.0, "f1": 0.0,
            "fpr": 0.0, "tpr": 0.0, "mcc": 0.0, "kappa": 0.0
        })
        
        global_stats = {
            "total_true_positives": 0, "total_false_positives": 0,
            "total_false_negatives": 0, "total_true_negatives": 0,
            "total_hits": 0, "total_misses": 0,
            "total_accuracy": 0.0, "total_precision": 0.0, "total_recall": 0.0,
            "total_f1": 0.0, "total_fpr": 0.0, "total_tpr": 0.0,
            "total_mcc": 0.0, "total_kappa": 0.0
        }

        # Build family-to-domain mapping from ground truth
        actual_families = {}
        valid_families = set()
        
        for filename in os.listdir(self.malicious_dir):
            if filename.endswith('.csv'):
                family = os.path.splitext(filename)[0]
                valid_families.add(family)
                
                try:
                    with open(os.path.join(self.malicious_dir, filename), 'r', encoding='utf-8') as f:
                        domains = [line.strip() for line in f if line.strip()]
                        for domain in domains:
                            actual_families[domain] = family
                except Exception as e:
                    print(f"Warning: Could not read family file {filename}: {e}")

        analyzed_domains = set()

        # Process classification results
        for result in result_list:
            try:
                parts = result.split('|')
                if len(parts) < 4:
                    raise ValueError("Insufficient parts for multiclass result")
                    
                domain = parts[0].strip()
                is_agd = parts[1].strip()
                predicted_family = parts[2].strip()
                
                # Validate domain and family
                if domain not in actual_families:
                    with open(self.format_error_multiclass_file, 'a', encoding='utf-8') as error_file:
                        error_file.write(f"Unknown domain: {result}\n")
                    continue

                if predicted_family not in valid_families:
                    with open(self.format_error_multiclass_file, 'a', encoding='utf-8') as error_file:
                        error_file.write(f"Invalid family: {result}\n")
                    continue

                # Skip duplicates
                if domain in analyzed_domains:
                    continue
                analyzed_domains.add(domain)

                actual_family = actual_families[domain]

                # Update statistics based on classification correctness
                if is_agd == "Y" and predicted_family == actual_family:
                    # Correct classification
                    family_stats[actual_family]["true_positives"] += 1
                    family_stats[actual_family]["hits"] += 1
                    global_stats["total_true_positives"] += 1
                    global_stats["total_hits"] += 1
                else:
                    # Incorrect classification
                    family_stats[actual_family]["false_negatives"] += 1
                    family_stats[actual_family]["misses"] += 1
                    global_stats["total_false_negatives"] += 1
                    global_stats["total_misses"] += 1

                    if is_agd == "Y" and predicted_family != actual_family:
                        # False positive for predicted family
                        family_stats[predicted_family]["false_positives"] += 1
                        global_stats["total_false_positives"] += 1

                    # Update true negatives for uninvolved families
                    for family in family_stats.keys():
                        if family != actual_family and family != predicted_family:
                            family_stats[family]["true_negatives"] += 1
                            global_stats["total_true_negatives"] += 1

            except (ValueError, IndexError) as e:
                with open(self.format_error_multiclass_file, 'a', encoding='utf-8') as error_file:
                    error_file.write(f"Format error: {result}\n")
                continue

        # Calculate metrics for each family
        for family, stats in family_stats.items():
            total = stats["hits"] + stats["misses"]
            
            # Basic accuracy
            if total > 0:
                stats["accuracy"] = (stats["hits"] / total) * 100

            # Precision, Recall, F1
            if (stats["true_positives"] + stats["false_positives"]) > 0:
                stats["precision"] = (stats["true_positives"] / 
                                    (stats["true_positives"] + stats["false_positives"])) * 100

            if (stats["true_positives"] + stats["false_negatives"]) > 0:
                stats["recall"] = (stats["true_positives"] / 
                                 (stats["true_positives"] + stats["false_negatives"])) * 100

            if (stats["precision"] + stats["recall"]) > 0:
                stats["f1"] = 2 * (stats["precision"] * stats["recall"]) / (stats["precision"] + stats["recall"])

            # Additional metrics (FPR, TPR, MCC, Kappa)
            if (stats["false_positives"] + stats["true_negatives"]) > 0:
                stats["fpr"] = (stats["false_positives"] / 
                              (stats["false_positives"] + stats["true_negatives"])) * 100

            stats["tpr"] = stats["recall"]  # TPR equals recall

            # MCC calculation
            numerator = ((stats["true_positives"] * stats["true_negatives"]) - 
                        (stats["false_positives"] * stats["false_negatives"]))
            denominator = math.sqrt((stats["true_positives"] + stats["false_positives"]) * 
                                  (stats["true_positives"] + stats["false_negatives"]) * 
                                  (stats["true_negatives"] + stats["false_positives"]) * 
                                  (stats["true_negatives"] + stats["false_negatives"]))
            stats["mcc"] = (numerator / denominator if denominator != 0 else 0) * 100

            # Kappa calculation
            total_cm = (stats["true_positives"] + stats["false_positives"] + 
                       stats["false_negatives"] + stats["true_negatives"])
            if total_cm > 0:
                p0 = (stats["true_positives"] + stats["true_negatives"]) / total_cm
                pe_pos = ((stats["true_positives"] + stats["false_positives"]) * 
                         (stats["true_positives"] + stats["false_negatives"])) / (total_cm * total_cm)
                pe_neg = ((stats["false_negatives"] + stats["true_negatives"]) * 
                         (stats["false_positives"] + stats["true_negatives"])) / (total_cm * total_cm)
                pe = pe_pos + pe_neg
                stats["kappa"] = ((p0 - pe) / (1 - pe) if pe != 1 else 0) * 100

        # Calculate global metrics similarly
        total_domains = global_stats["total_hits"] + global_stats["total_misses"]
        if total_domains > 0:
            global_stats["total_accuracy"] = (global_stats["total_hits"] / total_domains) * 100

        if (global_stats["total_true_positives"] + global_stats["total_false_positives"]) > 0:
            global_stats["total_precision"] = (global_stats["total_true_positives"] / 
                                             (global_stats["total_true_positives"] + 
                                              global_stats["total_false_positives"])) * 100

        if (global_stats["total_true_positives"] + global_stats["total_false_negatives"]) > 0:
            global_stats["total_recall"] = (global_stats["total_true_positives"] / 
                                           (global_stats["total_true_positives"] + 
                                            global_stats["total_false_negatives"])) * 100

        if (global_stats["total_precision"] + global_stats["total_recall"]) > 0:
            global_stats["total_f1"] = 2 * (global_stats["total_precision"] * global_stats["total_recall"]) / \
                                       (global_stats["total_precision"] + global_stats["total_recall"])

        # Additional global metrics
        if (global_stats["total_false_positives"] + global_stats["total_true_negatives"]) > 0:
            global_stats["total_fpr"] = (global_stats["total_false_positives"] / 
                                        (global_stats["total_false_positives"] + 
                                         global_stats["total_true_negatives"])) * 100

        global_stats["total_tpr"] = global_stats["total_recall"]

        # Global MCC
        numerator = ((global_stats["total_true_positives"] * global_stats["total_true_negatives"]) - 
                    (global_stats["total_false_positives"] * global_stats["total_false_negatives"]))
        denominator = math.sqrt((global_stats["total_true_positives"] + global_stats["total_false_positives"]) * 
                              (global_stats["total_true_positives"] + global_stats["total_false_negatives"]) * 
                              (global_stats["total_true_negatives"] + global_stats["total_false_positives"]) * 
                              (global_stats["total_true_negatives"] + global_stats["total_false_negatives"]))
        global_stats["total_mcc"] = (numerator / denominator if denominator != 0 else 0) * 100

        # Global Kappa
        total = (global_stats["total_true_positives"] + global_stats["total_false_positives"] + 
                global_stats["total_false_negatives"] + global_stats["total_true_negatives"])
        if total > 0:
            p0 = (global_stats["total_true_positives"] + global_stats["total_true_negatives"]) / total
            pe_pos = ((global_stats["total_true_positives"] + global_stats["total_false_positives"]) * 
                     (global_stats["total_true_positives"] + global_stats["total_false_negatives"])) / (total * total)
            pe_neg = ((global_stats["total_false_negatives"] + global_stats["total_true_negatives"]) * 
                     (global_stats["total_false_positives"] + global_stats["total_true_negatives"])) / (total * total)
            pe = pe_pos + pe_neg
            global_stats["total_kappa"] = ((p0 - pe) / (1 - pe) if pe != 1 else 0) * 100

        return dict(family_stats), global_stats

    # ========================================================================
    # REAL-WORLD DOMAIN ANALYSIS (Experiment 4)
    # ========================================================================

    def analyze_real_world_classifier(self, result_list: List[str]) -> Metrics:
        """
        Analyzes performance on real-world domains (assumed to be all benign).
        
        This method is used for Experiment 4 where we test the models on real
        DNS traffic from university logs, expecting all domains to be legitimate.

        Args:
            result_list (List[str]): Classification results in format "[DOMAIN]|[Y/N]|[CONFIDENCE]"

        Returns:
            Metrics: Performance metrics for benign classification
        """
        true_negatives = 0   # Correctly classified as benign (N)
        false_positives = 0  # Incorrectly classified as malicious (Y)
        analyzed_domains = set()

        for result in result_list:
            try:
                parts = result.split('|')
                if len(parts) < 3:
                    raise ValueError("Insufficient parts in result")
                    
                domain, classification = parts[0].strip(), parts[1].strip()

                # Skip duplicates
                if domain in analyzed_domains:
                    continue
                analyzed_domains.add(domain)

                if classification == 'N':  # Correctly identified as benign
                    true_negatives += 1
                else:  # False positive (flagged as malicious)
                    false_positives += 1

            except (ValueError, IndexError):
                with open(self.format_error_file, 'a', encoding='utf-8') as error_file:
                    error_file.write(f"Format error: {result}\n")
                continue

        # For real-world analysis: TP=0, FN=0 (no actual malicious domains)
        return self.compute_metrics(tp=0, fp=false_positives, fn=0, tn=true_negatives)

    # ========================================================================
    # CONFIDENCE INTERVAL ANALYSIS
    # ========================================================================

    def compute_metrics_with_confidence(self, tp: int, fp: int, fn: int, tn: int, 
                                      confidence: float = 0.95) -> 'ConfidenceMetrics':
        """
        Computes metrics with confidence intervals using Wilson score method.
        
        This method provides statistical confidence intervals for the metrics,
        which is crucial for understanding the reliability of our results.

        Args:
            tp, fp, fn, tn (int): Confusion matrix values
            confidence (float): Confidence level (default 0.95 for 95% CI)

        Returns:
            ConfidenceMetrics: Metrics with confidence intervals
        """
        try:
            from utils.confidence_metrics import ConfidenceMetrics, ConfidenceCalculator
            from sklearn.metrics import roc_curve, auc
        except ImportError:
            print("Warning: Confidence analysis requires sklearn and confidence_metrics module")
            # Return basic metrics without confidence intervals
            basic_metrics = self.compute_metrics(tp, fp, fn, tn)
            return basic_metrics

        # Calculate basic metrics
        basic_metrics = self.compute_metrics(tp, fp, fn, tn)
        total = tp + fp + fn + tn

        # Prepare data for ROC calculation
        y_true = [1] * (tp + fn) + [0] * (fp + tn)
        y_pred_binary = [1] * tp + [0] * fn + [1] * fp + [0] * tn
        y_scores = np.array(y_pred_binary, dtype=float)

        # Calculate AUC if both classes are present
        if len(set(y_true)) > 1:
            fpr, tpr, _ = roc_curve(y_true, y_scores)
            roc_auc = auc(fpr, tpr)
        else:
            roc_auc = 0.0

        # Calculate confidence intervals using Wilson method
        if total > 0:
            accuracy_ci = ConfidenceCalculator.wilson_confidence_interval(
                tp + tn, total, confidence)

            if (tp + fp) > 0:
                precision_ci = ConfidenceCalculator.wilson_confidence_interval(
                    tp, tp + fp, confidence)
            else:
                precision_ci = (0.0, 0.0)

            if (tp + fn) > 0:
                recall_ci = ConfidenceCalculator.wilson_confidence_interval(
                    tp, tp + fn, confidence)
            else:
                recall_ci = (0.0, 0.0)

            if (fp + tn) > 0:
                fpr_ci = ConfidenceCalculator.wilson_confidence_interval(
                    fp, fp + tn, confidence)
            else:
                fpr_ci = (0.0, 0.0)

            # AUC confidence interval using Hanley-McNeil approximation
            if roc_auc > 0:
                n_pos, n_neg = tp + fn, fp + tn
                if n_pos > 0 and n_neg > 0:
                    q1 = roc_auc / (2 - roc_auc)
                    q2 = (2 * roc_auc**2) / (1 + roc_auc)
                    se_auc = np.sqrt((roc_auc * (1 - roc_auc) + (n_pos - 1) * (q1 - roc_auc**2) + 
                                    (n_neg - 1) * (q2 - roc_auc**2)) / (n_pos * n_neg))

                    z_score = 1.96 if confidence == 0.95 else 2.576
                    auc_ci_lower = max(0.0, roc_auc - z_score * se_auc)
                    auc_ci_upper = min(1.0, roc_auc + z_score * se_auc)
                    auc_ci = (auc_ci_lower, auc_ci_upper)
                else:
                    auc_ci = (roc_auc, roc_auc)
            else:
                auc_ci = (0.0, 0.0)
        else:
            accuracy_ci = precision_ci = recall_ci = fpr_ci = auc_ci = (0.0, 0.0)

        # For now, use point estimates for complex metrics (F1, MCC, Kappa)
        f1_ci = (basic_metrics.f1_score, basic_metrics.f1_score)
        mcc_ci = (basic_metrics.mcc, basic_metrics.mcc)
        kappa_ci = (basic_metrics.kappa, basic_metrics.kappa)
        tpr_ci = recall_ci  # TPR equals recall

        return ConfidenceMetrics(
            accuracy=basic_metrics.accuracy, precision=basic_metrics.precision,
            recall=basic_metrics.recall, f1_score=basic_metrics.f1_score,
            fpr=basic_metrics.fpr, tpr=basic_metrics.tpr,
            mcc=basic_metrics.mcc, kappa=basic_metrics.kappa, auc=roc_auc,
            accuracy_ci=accuracy_ci, precision_ci=precision_ci, recall_ci=recall_ci,
            f1_ci=f1_ci, fpr_ci=fpr_ci, tpr_ci=tpr_ci,
            mcc_ci=mcc_ci, kappa_ci=kappa_ci, auc_ci=auc_ci
        )

    def analyze_classifier_with_confidence(self, result_list: List[str], 
                                         confidence: float = 0.95) -> Tuple['ConfidenceMetrics', 'ConfidenceMetrics', 'ConfidenceMetrics']:
        """
        Analyzes classifier with confidence intervals.

        Args:
            result_list (List[str]): Classification results
            confidence (float): Confidence level for intervals

        Returns:
            Tuple[ConfidenceMetrics, ConfidenceMetrics, ConfidenceMetrics]: 
                Metrics with CI for (malicious, benign, overall)
        """
        # Use same logic as analyze_classifier but return confidence metrics
        malicious_domains = set()
        benign_domains = set()
        analyzed_domains = set()

        # Load ground truth data
        for filename in os.listdir(self.malicious_dir):
            file_path = os.path.join(self.malicious_dir, filename)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    domains = [line.strip() for line in f if line.strip()]
                    malicious_domains.update(domains)
            except Exception as e:
                print(f"Warning: Could not read {filename}: {e}")

        try:
            with open(self.benign_file, 'r', encoding='utf-8') as f:
                domains = [line.strip() for line in f if line.strip()]
                benign_domains.update(domains)
        except Exception as e:
            print(f"Warning: Could not read benign file: {e}")

        # Initialize confusion matrix
        tp, fn, tn, fp = 0, 0, 0, 0

        # Process classification results
        for result in result_list:
            try:
                parts = result.split('|')
                if len(parts) < 3:
                    raise ValueError("Insufficient parts in result")
                    
                domain, classification = parts[0].strip(), parts[1].strip()
                
                if domain in analyzed_domains:
                    continue
                    
                if domain in malicious_domains:
                    if classification == 'Y':
                        tp += 1
                    else:
                        fn += 1
                    analyzed_domains.add(domain)
                elif domain in benign_domains:
                    if classification == 'N':
                        tn += 1
                    else:
                        fp += 1
                    analyzed_domains.add(domain)
                        
            except (ValueError, IndexError):
                with open(self.format_error_file, 'a', encoding='utf-8') as error_file:
                    error_file.write(f"Confidence analysis format error: {result}\n")
                continue

        # Compute metrics with confidence intervals
        malicious_metrics = self.compute_metrics_with_confidence(tp, 0, fn, 0, confidence)
        benign_metrics = self.compute_metrics_with_confidence(tn, 0, fp, 0, confidence)
        overall_metrics = self.compute_metrics_with_confidence(tp, fp, fn, tn, confidence)

        return malicious_metrics, benign_metrics, overall_metrics

    # ========================================================================
    # ROC ANALYSIS SUPPORT
    # ========================================================================

    def analyze_with_roc(self, file_path: str, size: int) -> Tuple[Metrics, Metrics, Metrics, Tuple[int, int, int, int]]:
        """
        Analyzes results and returns metrics plus confusion matrix for ROC analysis.
        
        This method supports the ROC analysis functionality described in the paper
        by providing the confusion matrix values needed for ROC curve generation.

        Args:
            file_path (str): Path to results file
            size (int): Number of classifications to analyze

        Returns:
            Tuple[Metrics, Metrics, Metrics, Tuple[int, int, int, int]]: 
                (malicious_metrics, benign_metrics, overall_metrics, (tp, fp, fn, tn))
        """
        domains, lines_read = self.read_file(file_path)
        lines = lines_read[0:size]
        
        # Standard analysis
        malicious_metrics, benign_metrics, overall_metrics = self.analyze_classifier(result_list=lines)
        
        # Extract confusion matrix values for ROC analysis
        malicious_domains = set()
        benign_domains = set()
        analyzed_domains = set()

        for filename in os.listdir(self.malicious_dir):
            file_path_mal = os.path.join(self.malicious_dir, filename)
            try:
                with open(file_path_mal, 'r', encoding='utf-8') as f:
                    domains_mal = [line.strip() for line in f if line.strip()]
                    malicious_domains.update(domains_mal)
            except Exception as e:
                print(f"Warning: Could not read {filename}: {e}")

        try:
            with open(self.benign_file, 'r', encoding='utf-8') as f:
                domains_ben = [line.strip() for line in f if line.strip()]
                benign_domains.update(domains_ben)
        except Exception as e:
            print(f"Warning: Could not read benign file: {e}")

        tp, fn, tn, fp = 0, 0, 0, 0

        for result in lines:
            try:
                parts = result.split('|')
                if len(parts) < 3:
                    continue
                    
                domain, classification = parts[0].strip(), parts[1].strip()
                
                if domain in analyzed_domains:
                    continue
                    
                if domain in malicious_domains:
                    if classification == 'Y':
                        tp += 1
                    else:
                        fn += 1
                    analyzed_domains.add(domain)
                elif domain in benign_domains:
                    if classification == 'N':
                        tn += 1
                    else:
                        fp += 1
                    analyzed_domains.add(domain)
            except (ValueError, IndexError):
                continue
        
        return malicious_metrics, benign_metrics, overall_metrics, (tp, fp, fn, tn)

    # ========================================================================
    # VISUALIZATION METHODS
    # ========================================================================

    def setup_plot_style(self):
        """
        Sets up consistent plotting style for all visualizations.
        This ensures all generated plots match the paper's aesthetic.
        """
        try:
            import matplotlib as mpl
            plt.switch_backend('agg')
            plt.style.use('seaborn-v0_8-paper')
            mpl.rcParams.update({
                'axes.linewidth': 1.5,
                'font.family': 'serif',
                'font.serif': ['Times New Roman'],
                'text.usetex': True,
                'figure.facecolor': 'white',
                'axes.facecolor': 'white',
                'font.size': 16,
                'savefig.dpi': 300,
                'text.latex.preamble': r'\usepackage{amsmath}'
            })
        except Exception as e:
            print(f"Warning: Could not set up plot style: {e}")

    def draw_progression(self, file_path: str, output_tag: str, metric: str = "f1_score"):
        """
        Generates convergence analysis plots for binary classification.
        
        This method creates the convergence plots shown in the paper, demonstrating
        how model performance stabilizes as more samples are processed.

        Args:
            file_path (str): Path to results file
            output_tag (str): Tag for output filename
            metric (str): Metric to plot (default: "f1_score")
        """
        self.setup_plot_style()

        try:
            _, data = self.read_file(file_path)
            progression = range(100, min(50001, len(data) + 1), 200)
            scores = []
            actual_sizes = []

            for size in progression:
                subset = data[:size]
                if not subset:
                    break
                    
                try:
                    _, _, metrics = self.analyze_classifier(subset)
                    score_value = getattr(metrics, metric, 0.0)
                    scores.append(score_value * 100.0)  # Convert to percentage
                    actual_sizes.append(len(subset))
                except Exception as e:
                    print(f"Warning: Error processing size {size}: {e}")
                    continue

            if not scores:
                print("Warning: No valid scores calculated for progression plot")
                return

            sizes_k = [size/1000 for size in actual_sizes]

            fig, ax = plt.subplots(figsize=(8, 6))
            ax.plot(sizes_k, scores, 
                    marker='o', linestyle='-', 
                    markersize=6, linewidth=1.5,
                    label=metric.replace('_', ' ').title())

            ax.set_xlabel('Number of Domain Samples', fontsize=16.5)
            ax.set_ylabel(f'{metric.replace("_", " ").title()} (\\%)', fontsize=16.5)
            ax.grid(True, ls='-', alpha=0.2)
            ax.legend(frameon=False, fontsize=15)
            ax.tick_params(width=1.5, labelsize=14)

            plt.tight_layout()
            
            # Ensure output directory exists
            os.makedirs("plots", exist_ok=True)
            output_path = os.path.join("plots", f"{output_tag}_{metric}.pdf")
            plt.savefig(output_path, format='pdf', bbox_inches='tight', dpi=300)
            plt.close()
            print(f"Convergence plot saved to: {output_path}")
            
        except Exception as e:
            print(f"Error generating progression plot: {e}")

    def draw_progression_double(self, file_path1: str, file_path2: str, 
                              output_tag: str, metric: str = "f1_score"):
        """
        Generates comparison plots between two prompting strategies.
        
        This method creates the comparison plots shown in the paper between
        different prompting approaches (e.g., P1 vs P2).

        Args:
            file_path1 (str): Path to first results file (P1)
            file_path2 (str): Path to second results file (P2)
            output_tag (str): Tag for output filename
            metric (str): Metric to plot (default: "f1_score")
        """
        self.setup_plot_style()

        try:
            _, data1 = self.read_file(file_path1)
            _, data2 = self.read_file(file_path2)
            
            max_size = min(len(data1), len(data2), 50000)
            progression = range(100, max_size + 1, 200)
            
            scores1, scores2 = [], []
            actual_sizes = []

            for size in progression:
                subset1 = data1[:size]
                subset2 = data2[:size]
                
                if not subset1 or not subset2:
                    break
                    
                try:
                    _, _, metrics1 = self.analyze_classifier(subset1)
                    _, _, metrics2 = self.analyze_classifier(subset2)
                    
                    score1 = getattr(metrics1, metric, 0.0) * 100.0
                    score2 = getattr(metrics2, metric, 0.0) * 100.0
                    
                    scores1.append(score1)
                    scores2.append(score2)
                    actual_sizes.append(len(subset1))
                except Exception as e:
                    print(f"Warning: Error processing size {size}: {e}")
                    continue

            if not scores1 or not scores2:
                print("Warning: No valid scores calculated for comparison plot")
                return

            sizes_k = [size/1000 for size in actual_sizes]

            fig, ax = plt.subplots(figsize=(8, 6))

            ax.plot(sizes_k, scores1, 
                    marker='o', linestyle='-', 
                    markersize=4, linewidth=1.5,
                    label=r'$P_1')
                    
            ax.plot(sizes_k, scores2, 
                    marker='s', linestyle='-', 
                    markersize=4, linewidth=1.5,
                    label=r'$P_2')

            ax.set_xlabel('Number of Domain Samples', fontsize=16.5)
            ax.set_ylabel(f'{metric.replace("_", " ").title()} (\\%)', fontsize=16.5)
            ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: f'{int(x)}k' if x.is_integer() else f'{x}k'))
            ax.grid(False)
            ax.legend(frameon=False, fontsize=15)
            ax.tick_params(width=1.5, labelsize=14)

            plt.tight_layout()
            
            os.makedirs("plots", exist_ok=True)
            output_path = os.path.join("plots", f"{output_tag}_{metric}.pdf")
            plt.savefig(output_path, format='pdf', bbox_inches='tight', dpi=300)
            plt.close()
            print(f"Comparison plot saved to: {output_path}")
            
        except Exception as e:
            print(f"Error generating comparison plot: {e}")

    def draw_progression_multiclass(self, file_path: str, output_tag: str):
        """
        Generates convergence plots for multiclass family classification.

        Args:
            file_path (str): Path to results file
            output_tag (str): Tag for output filename
        """
        self.setup_plot_style()

        try:
            _, data = self.read_file(file_path)
            progression = range(100, min(50001, len(data) + 1), 200)
            f1_scores = []
            actual_sizes = []

            for size in progression:
                subset = data[:size]
                if not subset:
                    break
                    
                try:
                    _, global_metrics = self.analyze_family_metrics(subset)
                    f1_scores.append(global_metrics.get("total_f1", 0.0))
                    actual_sizes.append(len(subset))
                except Exception as e:
                    print(f"Warning: Error processing multiclass size {size}: {e}")
                    continue

            if not f1_scores:
                print("Warning: No valid F1 scores calculated for multiclass plot")
                return

            sizes_k = [size/1000 for size in actual_sizes]

            fig, ax = plt.subplots(figsize=(8, 6))
            ax.plot(sizes_k, f1_scores, 
                    marker='o', linestyle='-', 
                    markersize=6, linewidth=1.5,
                    label='F1-Score')

            ax.set_xlabel('Number of Domain Samples', fontsize=16.5)
            ax.set_ylabel('F1-Score (\\%)', fontsize=16.5)
            ax.grid(True, ls='-', alpha=0.2)
            ax.legend(frameon=False, fontsize=15)
            ax.tick_params(width=1.5, labelsize=14)

            plt.tight_layout()
            
            os.makedirs("plots", exist_ok=True)
            output_path = os.path.join("plots", f"{output_tag}_multiclass.pdf")
            plt.savefig(output_path, format='pdf', bbox_inches='tight', dpi=300)
            plt.close()
            print(f"Multiclass plot saved to: {output_path}")
            
        except Exception as e:
            print(f"Error generating multiclass plot: {e}")

    def draw_progression_real_world(self, file_path: str, output_tag: str):
        """
        Generates convergence plots for real-world domain analysis.

        Args:
            file_path (str): Path to results file
            output_tag (str): Tag for output filename
        """
        self.setup_plot_style()

        try:
            _, data = self.read_file(file_path)
            progression = range(100, min(50001, len(data) + 1), 200)
            scores = []
            actual_sizes = []

            for size in progression:
                subset = data[:size]
                if not subset:
                    break
                    
                try:
                    metrics = self.analyze_real_world_classifier(subset)
                    scores.append(metrics.accuracy * 100.0)
                    actual_sizes.append(len(subset))
                except Exception as e:
                    print(f"Warning: Error processing real-world size {size}: {e}")
                    continue

            if not scores:
                print("Warning: No valid scores calculated for real-world plot")
                return

            sizes_k = [size/1000 for size in actual_sizes]

            fig, ax = plt.subplots(figsize=(8, 6))
            ax.plot(sizes_k, scores, 
                    marker='o', linestyle='-', 
                    markersize=6, linewidth=1.5,
                    label='Accuracy')

            ax.set_xlabel('Number of Domain Samples', fontsize=16.5)
            ax.set_ylabel('Accuracy (\\%)', fontsize=16.5)
            ax.grid(True, ls='-', alpha=0.2)
            ax.legend(frameon=False, fontsize=15)
            ax.tick_params(width=1.5, labelsize=14)

            plt.tight_layout()
            
            os.makedirs("plots", exist_ok=True)
            output_path = os.path.join("plots", f"{output_tag}_real_world.pdf")
            plt.savefig(output_path, format='pdf', bbox_inches='tight', dpi=300)
            plt.close()
            print(f"Real-world plot saved to: {output_path}")
            
        except Exception as e:
            print(f"Error generating real-world plot: {e}")

    # ========================================================================
    # PUBLIC API METHODS
    # ========================================================================

    def check_domains(self, file_path: str, output_path: str) -> bool:
        """
        Checks if all domains have been classified.
        
        This is the main validation method used by the main script to ensure
        complete classification before proceeding with analysis.

        Args:
            file_path (str): Path to results file
            output_path (str): Path to save any missing domains

        Returns:
            bool: True if all domains are classified, False otherwise
        """
        try:
            domains, classifications = self.read_file(file_path)
            return self.validate_domains(domains, classifications, output_file=output_path)
        except Exception as e:
            print(f"Error checking domains: {e}")
            return False

    def analyze(self, file_path: str, size: int) -> Tuple[Metrics, Metrics, Metrics]:
        """
        Main analysis method for binary classification (Experiments 1-2).

        Args:
            file_path (str): Path to results file
            size (int): Number of classifications to analyze

        Returns:
            Tuple[Metrics, Metrics, Metrics]: (malicious_metrics, benign_metrics, overall_metrics)
        """
        try:
            domains, lines_read = self.read_file(file_path)
            lines = lines_read[0:size]
            return self.analyze_classifier(result_list=lines)
        except Exception as e:
            print(f"Error in binary analysis: {e}")
            # Return empty metrics in case of error
            empty_metrics = self.compute_metrics(0, 0, 0, 0)
            return empty_metrics, empty_metrics, empty_metrics

    def analyze_multiclass(self, file_path: str, size: int) -> Tuple[Dict, Dict]:
        """
        Main analysis method for multiclass family classification (Experiment 3).

        Args:
            file_path (str): Path to results file
            size (int): Number of classifications to analyze

        Returns:
            Tuple[Dict, Dict]: (family_statistics, global_statistics)
        """
        try:
            domains, lines_read = self.read_file(file_path)
            lines = lines_read[0:size]
            return self.analyze_family_metrics(result_list=lines)
        except Exception as e:
            print(f"Error in multiclass analysis: {e}")
            return {}, {}

    def analyze_real_world_results(self, file_path: str, size: int) -> Metrics:
        """
        Main analysis method for real-world domain analysis (Experiment 4).

        Args:
            file_path (str): Path to results file
            size (int): Number of classifications to analyze

        Returns:
            Metrics: Performance metrics for benign classification
        """
        try:
            domains, lines_read = self.read_file(file_path)
            lines = lines_read[0:size]
            return self.analyze_real_world_classifier(result_list=lines)
        except Exception as e:
            print(f"Error in real-world analysis: {e}")
            return self.compute_metrics(0, 0, 0, 0)

    def analyze_with_confidence(self, file_path: str, size: int, 
                              confidence: float = 0.95) -> Tuple['ConfidenceMetrics', 'ConfidenceMetrics', 'ConfidenceMetrics']:
        """
        Analysis with confidence intervals for binary classification.

        Args:
            file_path (str): Path to results file
            size (int): Number of classifications to analyze
            confidence (float): Confidence level for intervals

        Returns:
            Tuple[ConfidenceMetrics, ConfidenceMetrics, ConfidenceMetrics]: 
                Metrics with CI for (malicious, benign, overall)
        """
        try:
            domains, lines_read = self.read_file(file_path)
            lines = lines_read[0:size]
            return self.analyze_classifier_with_confidence(result_list=lines, confidence=confidence)
        except Exception as e:
            print(f"Error in confidence analysis: {e}")
            # Return basic metrics without confidence intervals
            basic_metrics = self.compute_metrics(0, 0, 0, 0)
            return basic_metrics, basic_metrics, basic_metrics